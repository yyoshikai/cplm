{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "BindGPTはどのような工夫をしていたか？\n",
    "    Flash Attention 2\n",
    "        PyTorch 2.2では標準\n",
    "    DeepSpeed\n",
    "    HuggingFaceを使っていた。\n",
    "    sampling時: key-value caching\n",
    "        DeepSpeed ZeRO Stage-3d\n",
    "        …single GPUでは必要なさそう。\n",
    "\n",
    "どう実装するか\n",
    "    HuggingFace\n",
    "        使い方が分からない\n",
    "        カスタム性が不明\n",
    "        そこにしかない機能があるかもしれない。\n",
    "    PyTorch\n",
    "        古い。\n",
    "        重みの初期化とかはhuggingfaceの方がいいかもしれない。\n",
    "\n",
    "パラメータ\n",
    "    層数: 15\n",
    "    次元数: 768\n",
    "    head数: 12\n",
    "    ffの次元数: ?\n",
    "        とりあえず4倍とする。\n",
    "    dropout: ?\n",
    "        0.1\n",
    "    activation: ?\n",
    "        gelu\n",
    "…そんなに多くない気がする。\n",
    "\n",
    "To speedup\n",
    "and stabilize pretraining, we use large batch training (Keskar et al., 2017) with 1.6M tokens\n",
    "per one training step. We found this many tokens per batch to be important for stable\n",
    "training in this task even with smaller learning rates. The detailed description of the training\n",
    "implementation is provided in Appendix G. \n",
    "We use learning rate\n",
    "warmup of 2000 steps, followed by\n",
    "cosine annealing of the learning rate.\n",
    "The maximal learning rate during pretraining is 10−3\n",
    "regardless of the model size\n",
    "\n",
    "We use AdamW optimizer (Loshchilov & Hutter,\n",
    "2019) with a weight decay factor of 10−2\n",
    ". We use gradient clipping with the maximal grad\n",
    "norm of 1.0. The pretraining takes around 55k optimization steps over 36 hours on one\n",
    "compute node with 8 A6000 GPUs. We employ Flash-Attention2 Dao (2023) and DeepSpeed\n",
    "optimization accelerator. To use more performant tensor cores, we train with mixed precision where computation is done within the bfloat16 datatype.\n",
    "\n",
    "batch sizeが多そうだが, モデルは乗りそうなのでDataParallelのみでよい？\n",
    "\n",
    "\n",
    "どう学習するか？\n",
    "    やりたいこと:\n",
    "        max_lengthの設定\n",
    "            bucketing...?\n",
    "                max_lengthの設定, 大きいデータが来たら止めるなども候補。\n",
    "        DataParallel\n",
    "            ZeROは必ずしも必要ない。\n",
    "    方法:\n",
    "        この場で適当に実装する。\n",
    "            huggingfaceを使う\n",
    "                まあ急いでいるわけではないので, 勉強も兼ねて。\n",
    "            pytorchのみを使う\n",
    "        旧modelsを使う\n",
    "        新modelsを使う\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import\n",
    "import lmdb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "sys.path.append('/workspace')\n",
    "from models.datasets.tokenizer import get_toker\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "from biopandas.pdb import PandasPdb\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayTokenizer():\n",
    "    def __init__(self, n_max, vmin, vsup, fdim, offset=0):\n",
    "        ooffset = offset\n",
    "        self.start_token = offset\n",
    "        self.end_token = offset+1\n",
    "        offset += 2\n",
    "\n",
    "        self.n_offset = offset\n",
    "        self.n_max = n_max\n",
    "        offset += n_max+1\n",
    "\n",
    "        self.i_offset = offset\n",
    "        self.fdim = fdim\n",
    "        self.vmin = vmin\n",
    "        self.dmax = vsup - 0.1**(self.fdim+1) - vmin\n",
    "        offset += vsup-vmin\n",
    "\n",
    "        self.f_offset = offset\n",
    "        offset += 10**self.fdim\n",
    "        self.voc_size = offset - ooffset\n",
    "\n",
    "    def tokenize(self, array: np.ndarray):\n",
    "        tokens = []\n",
    "        tokens.append(self.start_token)\n",
    "        # n\n",
    "        tokens.append(min(len(array), self.n_max)+self.n_offset)\n",
    "\n",
    "        # values\n",
    "        for v in array.ravel():\n",
    "            v = max(0, min(self.dmax, v-self.vmin))\n",
    "            f, i = math.modf(v)\n",
    "            tokens.append(int(i)+self.i_offset)\n",
    "            tokens.append(int(f*10**self.fdim)+self.f_offset)\n",
    "\n",
    "        tokens.append(self.end_token)\n",
    "        return tokens\n",
    "\n",
    "class UniMolLigandDataset(Dataset):\n",
    "    def __init__(self, lmdb_path, voc_path, max_length):\n",
    "        self.env = None\n",
    "\n",
    "        self.env = lmdb.open(\n",
    "            lmdb_path,\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=256,\n",
    "        )\n",
    "        self.txn = self.env.begin()\n",
    "        self.keys = list(self.txn.cursor().iternext(values=False))\n",
    "        self.smi_toker = get_toker(voc_path)\n",
    "        self.coord_toker = ArrayTokenizer(100, -20, 20, 3, self.smi_toker.voc_size)\n",
    "        self.coord_rng = np.random.default_rng(seed=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.txn.get(self.keys[index])\n",
    "        data = pickle.loads(data)\n",
    "        tokens = self.smi_toker.tokenize(data['smi'])[:-1]\n",
    "        tokens += self.coord_toker.tokenize(self.coord_rng.choice(data['coordinates']))        \n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.env is not None:\n",
    "            self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "dataset = UniMolLigandDataset(\"/workspace/cheminfodata/unimol/ligands/valid.lmdb\", \"/workspace/cheminfodata/vocs/bchirals2.txt\", max_length)\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 773/773 [00:07<00:00, 106.54it/s]\n"
     ]
    }
   ],
   "source": [
    "collate_fn = functools.partial(pad_sequence, padding_value=dataset.smi_toker.pad_token)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=4, \n",
    "    collate_fn=collate_fn, pin_memory=True, \n",
    ")\n",
    "for batch in tqdm(loader):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98844/98844 [00:27<00:00, 3637.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dataset))):\n",
    "    data = dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "d_model = 768\n",
    "nhead = 12\n",
    "dim_feedforward = d_model*4\n",
    "dropout = 0.1\n",
    "num_layers = 15\n",
    "\n",
    "layer = nn.TransformerDecoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout, \n",
    "    activation='gelu',\n",
    "    norm_first=True\n",
    ")\n",
    "norm = nn.LayerNorm(d_model, elementwise_affine=False)\n",
    "model = nn.TransformerDecoder(layer, num_layers, norm)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model, lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1. experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### アミノ酸配列を復元できないか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmdb_path = \"/workspace/cheminfodata/unimol/pockets/valid.lmdb\"\n",
    "env = lmdb.open(\n",
    "    lmdb_path,\n",
    "    subdir=False,\n",
    "    readonly=True,\n",
    "    lock=False,\n",
    "    readahead=False,\n",
    "    meminit=False,\n",
    "    max_readers=256,\n",
    ")\n",
    "txn = env.begin()\n",
    "keys = list(txn.cursor().iternext(values=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164409/164409 [00:10<00:00, 15292.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "residuess = defaultdict(int)\n",
    "flg = False\n",
    "atom_types = set()\n",
    "for key in tqdm(keys):\n",
    "    data = pickle.loads(txn.get(key))\n",
    "    atoms = data['atoms']\n",
    "    atom_types |= set(list(data['atoms']))\n",
    "\n",
    "    residue = set()\n",
    "    for i, (a, r) in enumerate(zip(atoms, data['side'])):\n",
    "        if a == 'OXT': a = 'O'\n",
    "        if len(a) >= 2 and a[1].isdigit():\n",
    "            break\n",
    "        if a == 'NA': break\n",
    "        if a[0] == 'H': continue\n",
    "\n",
    "        if r == 0:\n",
    "            if a in residue:\n",
    "                residue = ','.join(sorted(residue))\n",
    "\n",
    "                residuess[residue]+=1\n",
    "                residue = set()\n",
    "\n",
    "        residue.add(a)\n",
    "    if len(residue) > 0:\n",
    "        residuess[','.join(sorted(residue))]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', \"C1'\", 'C1B', 'C1D', 'C2', \"C2'\", 'C2A', 'C2B', 'C2D', 'C2N',\n",
       "       \"C3'\", 'C3B', 'C3D', 'C3N', 'C4', \"C4'\", 'C4A', 'C4B', 'C4D',\n",
       "       'C4N', 'C5', \"C5'\", 'C5A', 'C5B', 'C5D', 'C5N', 'C6', 'C6A', 'C6N',\n",
       "       'C7N', 'C8', 'C8A', 'CA', 'CB', 'CD', 'CD1', 'CD2', 'CE', 'CE1',\n",
       "       'CE2', 'CE3', 'CG', 'CG1', 'CG2', 'CH2', 'CL', 'CU', 'CZ', 'CZ2',\n",
       "       'CZ3', 'H', 'H1', \"H1'\", 'H1B', 'H1D', 'H2', \"H2'\", 'H2A', 'H2B',\n",
       "       'H2D', 'H2N', 'H3', \"H3'\", 'H3B', 'H3D', \"H4'\", 'H42N', 'H4B',\n",
       "       'H4D', 'H4N', \"H5'\", \"H5''\", 'H51A', 'H51N', 'H52A', 'H52N', 'H5N',\n",
       "       'H61A', 'H62A', 'H6N', 'H71N', 'H72N', 'H8', 'H8A', 'HA', 'HA2',\n",
       "       'HA3', 'HB', 'HB1', 'HB2', 'HB3', 'HD1', 'HD11', 'HD12', 'HD13',\n",
       "       'HD2', 'HD21', 'HD22', 'HD23', 'HD3', 'HE', 'HE1', 'HE2', 'HE21',\n",
       "       'HE22', 'HE3', 'HG', 'HG1', 'HG11', 'HG12', 'HG13', 'HG2', 'HG21',\n",
       "       'HG22', 'HG23', 'HG3', 'HH', 'HH11', 'HH12', 'HH2', 'HH21', 'HH22',\n",
       "       'HN1', 'HN21', 'HN22', \"HO2'\", 'HO2A', 'HO2N', \"HO3'\", 'HO3A',\n",
       "       'HO3N', 'HOA2', 'HOB2', 'HOB3', 'HZ', 'HZ1', 'HZ2', 'HZ3', 'N',\n",
       "       'N1', 'N1A', 'N1N', 'N2', 'N3', 'N3A', 'N6A', 'N7', 'N7A', 'N7N',\n",
       "       'N9', 'N9A', 'NA', 'ND1', 'ND2', 'NE', 'NE1', 'NE2', 'NH1', 'NH2',\n",
       "       'NZ', 'O', 'O1A', 'O1B', 'O1N', \"O2'\", 'O2A', 'O2B', 'O2D', 'O2N',\n",
       "       'O3', \"O3'\", 'O3A', 'O3B', 'O3D', \"O4'\", 'O4B', 'O4D', \"O5'\",\n",
       "       'O5B', 'O5D', 'O6', 'O7N', 'OD1', 'OD2', 'OE1', 'OE2', 'OG', 'OG1',\n",
       "       'OH', 'OXT', 'SD', 'SG'], dtype='<U4')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sorted(atom_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(residuess))\n",
    "df = pd.Series(residuess)\n",
    "df.sort_values()[::-1].to_csv(\"residues.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms_1 = [a for a in atoms2 if len(a) == 1]\n",
    "atoms_i = [a for a in atoms2 if len(a) >= 2 and a[1].isdigit()]\n",
    "atoms_a = [a for a in atoms2 if len(a) >= 2 and not a[1].isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD1', 'CD2', 'CE1', 'CE2', 'CZ',\n",
       "       'OH', 'H', 'HA', 'HB2', 'HB3', 'HD1', 'HD2', 'HE1', 'HE2', 'HH',\n",
       "       'N', 'CA', 'C', 'O', 'CB', 'CG', 'CD', 'OE1', 'OE2', 'H', 'HA',\n",
       "       'HB2', 'HB3', 'HG2', 'HG3', 'N', 'CA', 'C', 'O', 'CB', 'CG', 'SD',\n",
       "       'CE', 'H', 'HA', 'HB2', 'HB3', 'HG2', 'HG3', 'HE1', 'HE2', 'HE3',\n",
       "       'N', 'CA', 'C', 'O', 'CB', 'OG', 'H', 'HA', 'HB2', 'HB3', 'HG',\n",
       "       'N', 'CA', 'C', 'O', 'CB', 'CG', 'H', 'HA', 'HB2', 'HB3', 'ND1',\n",
       "       'CD2', 'CE1', 'NE2', 'HD1', 'HD2', 'HE1', 'N', 'CA', 'C', 'O',\n",
       "       'CB', 'CG', 'OD1', 'OD2', 'H', 'HA', 'HB2', 'HB3', 'CU', 'O', 'H1',\n",
       "       'H2', 'O', 'H1', 'H2', 'O', 'H1', 'H2'], dtype='<U3')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NA',\n",
       " 'O',\n",
       " 'H1',\n",
       " 'H2',\n",
       " 'O',\n",
       " 'H1',\n",
       " 'H2',\n",
       " 'O',\n",
       " 'H1',\n",
       " 'H2',\n",
       " 'O',\n",
       " 'H1',\n",
       " 'H2',\n",
       " 'O',\n",
       " 'H1',\n",
       " 'H2']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.where(np.array(atoms) == 'NA')\n",
    "atoms[261:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"C1'\" 'C1B' 'C1D' 'C2' \"C2'\" 'C2A' 'C2B' 'C2D' 'C2N' \"C3'\" 'C3B' 'C3D'\n",
      " 'C3N' 'C4' \"C4'\" 'C4A' 'C4B' 'C4D' 'C4N' 'C5' \"C5'\" 'C5A' 'C5B' 'C5D'\n",
      " 'C5N' 'C6' 'C6A' 'C6N' 'C7N' 'C8' 'C8A' 'H1' \"H1'\" 'H1B' 'H1D' 'H2' \"H2'\"\n",
      " 'H2A' 'H2B' 'H2D' 'H2N' 'H3' \"H3'\" 'H3B' 'H3D' \"H4'\" 'H42N' 'H4B' 'H4D'\n",
      " 'H4N' \"H5'\" \"H5''\" 'H51A' 'H51N' 'H52A' 'H52N' 'H5N' 'H61A' 'H62A' 'H6N'\n",
      " 'H71N' 'H72N' 'H8' 'H8A' 'N1' 'N1A' 'N1N' 'N2' 'N3' 'N3A' 'N6A' 'N7'\n",
      " 'N7A' 'N7N' 'N9' 'N9A' 'O1A' 'O1B' 'O1N' \"O2'\" 'O2A' 'O2B' 'O2D' 'O2N'\n",
      " 'O3' \"O3'\" 'O3A' 'O3B' 'O3D' \"O4'\" 'O4B' 'O4D' \"O5'\" 'O5B' 'O5D' 'O6'\n",
      " 'O7N']\n"
     ]
    }
   ],
   "source": [
    "print(np.array(atoms_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA' 'CB' 'CD' 'CD1' 'CD2' 'CE' 'CE1' 'CE2' 'CE3' 'CG' 'CG1' 'CG2' 'CH2'\n",
      " 'CL' 'CU' 'CZ' 'CZ2' 'CZ3' 'HA' 'HA2' 'HA3' 'HB' 'HB1' 'HB2' 'HB3' 'HD1'\n",
      " 'HD11' 'HD12' 'HD13' 'HD2' 'HD21' 'HD22' 'HD23' 'HD3' 'HE' 'HE1' 'HE2'\n",
      " 'HE21' 'HE22' 'HE3' 'HG' 'HG1' 'HG11' 'HG12' 'HG13' 'HG2' 'HG21' 'HG22'\n",
      " 'HG23' 'HG3' 'HH' 'HH11' 'HH12' 'HH2' 'HH21' 'HH22' 'HN1' 'HN21' 'HN22'\n",
      " \"HO2'\" 'HO2A' 'HO2N' \"HO3'\" 'HO3A' 'HO3N' 'HOA2' 'HOB2' 'HOB3' 'HZ' 'HZ1'\n",
      " 'HZ2' 'HZ3' 'NA' 'ND1' 'ND2' 'NE' 'NE1' 'NE2' 'NH1' 'NH2' 'NZ' 'OD1'\n",
      " 'OD2' 'OE1' 'OE2' 'OG' 'OG1' 'OH' 'OXT' 'SD' 'SG']\n"
     ]
    }
   ],
   "source": [
    "print(np.array(atoms_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu117-2.0.0",
   "language": "python",
   "name": "cu117-2.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
