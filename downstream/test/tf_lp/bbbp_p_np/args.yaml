batch_sizes:
- 32
- 64
data: bbbp
ddp_size: 1
deterministic: false
gpu_size: 42949672960.0
gpu_size_gb: 40.0
local_batch_size: 16
lrs:
- 5.0e-05
- 8.0e-05
- 0.0001
- 0.0004
- 0.0005
n_epochs:
- 40
- 60
- 80
- 100
n_trials: 100
no_commit: false
num_workers: 4
patience: 5
pretrain_dir: downstream/finetune/results/251001_20000/cls/tf_lp
pretrain_opt: 900
pretrain_patience_val: 10
sdp_kernel: FLASH
seed: 0
studyname: test/tf_lp
task: p_np
test: null
warmup_ratios:
- 0.0
- 0.06
- 0.1
weight_decay: 0.01
