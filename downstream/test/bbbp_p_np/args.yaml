batch_sizes:
- 32
- 64
data: bbbp
ddp_size: 1
deterministic: false
gpu_size: 42949672960.0
gpu_size_gb: 40.0
lrs:
- 5.0e-05
- 8.0e-05
- 0.0001
- 0.0004
- 0.0005
n_epochs:
- 40
- 60
- 80
- 100
n_trials: 100
no_commit: false
num_workers: 4
patience: 5
pretrain_dir: downstream/finetune/results/251001_20000/cls/mamba_lp
pretrain_opt: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - i8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  WAIAAAAAAAA=
pretrain_patience_val: 10
sdp_kernel: FLASH
seed: 0
studyname: test
task: p_np
test: null
warmup_ratios:
- 0.0
- 0.06
- 0.1
weight_decay: 0.01
